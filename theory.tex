\section{Theory}

\subsection{Lattice Boltzmann Model (\textit{Jens Bang})}
The Lattice Boltzmann Model (LBM) is a simplified version of the Boltzmann equation, which simulates the behaviour of fluid flows. The simplification consists of limiting the particles to only occupy certain points in space (vertices in a lattice), and to only travel along specified directional vectors, with constant speed. 

In this way the Lattice Boltzmann Model (LBM) describe simple fluids (gas and liquids), i.e. it ignores thermal effects and tracers. The LBM simulates movements in fluids by looking at particles found at points in a lattice at discrete time-steps. There are different versions of the LBM using either 2-dimensional or 3-dimensional lattices.

Each point in the lattice has a set of state-variables attached, describing the state of the particle found at that point. Each point also has a set of fixed directions, along which the particle can travel. In a 2-dimensional lattice there are normally 9 directions, while in a 3-dimensional lattice you see either 15 or 19 directions. These 3 setups are normally referred to as D2Q9, D3Q15 and D3Q19.

\insfig{./images/d2q9_d3q19.png}{0.5\textwidth}{LBM directions (TODO: Insert reference to Scholarpedia)}{lbmdirections}

The LBM divides the flow of gasses or fluids into small, discrete time-steps. For each time-step the algorithm traverses all lattice points and for each point calculates the velocity and direction the particle travels in, as well as handling any collisions between particles.

\subsubsection{Using the LBM}
Since the LBM is especially well suited to simulate flows around even very complex geometric structures, it has a wide variety of practical uses. Among these are:
\begin{itemize}
\item Simulating pore-scale processes in porous media [TODO Citation: BrittSBChristensenPhD-Thesis.pdf]
\item Wind turbines [TODO Citation needed]
\item Bridge pillars [TODO Citation needed]
\item Others?
\end{itemize}

\subsection{CPU vs. GPU}
  Different goals produce different designs !   GPU assumes work load is highly parallel !   CPU must be good at everything, parallel or not
!   CPU: minimize latency experienced by 1 thread !   big on-chip caches !   sophisticated control logic
!   GPU: maximize throughput of all threads
!  
!   !  
threads in flight limited by resources => lots of resources (registers, bandwidth, etc.)
multithreading can hide latency => skip the big caches share control logic across many threads

\subsection{Parallel processing}

\subsection{CUDA (\textit{Jacob Salomonsen})}
In the beginning, GPU's had only very limited purposes. Mostly generating real-time graphics for games, but also in a few cases for production and scientific applications. This has changed since the proliferation of the pixel shader. A pixel shader basically produces a color for every point on a screen, by taking into account the (x,y) position of the pixel, the light settings of a scene, the material properties of objects in the scene and so on. Early researchers found that this ability to perform computations on each pixel could be harnessed to other uses. Since pixel shaders are completely controlled by the programmer, the possibility of simply giving a GPU data as input, instead of a scene to render, was within reach. This would mark the beginning of the general purpose GPU.

The general purpose GPU did however lack a platform for developers to build upon, since learning to program pixel shaders required previous knowledge about either openGL or DirectX. Also even when knowing these frameworks, the model would lack the perspective of general purpose computing and instead be focused on generating graphics, where the general purpose computing aspect would be achieved by "cheating" the GPU into treating data as if it was a scene to be rasterized.

Enter CUDA. CUDA is nVidia's way of introducing general purpose GPU programming to a wider audience. CUDA utilizes the parallel nature of the GPU, allowing a low end computer to process data in a different, and some times more efficient, way.

The CUDA API exposes a set of tags, which allows a programmer to easily specify what methods, in otherwise ordinary C-code, should be executed on the GPU. The way CUDA makes it possible is by using a preprocessor which parses the source code and makes different files for the individual architectures. Thus the normal C-code goes to whichever compiler the user chooses, and the parallel section of the code goes to a compiler specifically designed to generate PTX-code. PTX-code is essentially assembly which is not specific to any GPU. The PTX code is then translated into assembly code specifically targeted at the GPU.

This effectively separates the tedium of writing assembly code directly aimed at a specific GPU-architecture, or writing pixel shaders via graphics APIs.

\subsubsection{CUDA programming model}
The CUDA programming model efficiently supports the steps needed to be taken, in order to make a process parallel. The model is arranged so a decomposition of the problem area is naturally ingrained in the programming procedure, by having the programmer split the problem into parts. Since the GPU is capable of launching many threads, it is necessary to have control of what thread is launched where. Therefore nVidia has structured execution of code in a way, that both allows an easy intuition of the GPU-architecture, as well as supporting the steps in making a process parallel.

The code to be executed on the GPU, \emph{the kernel}, launches a grid which contains blocks of threads. A kernel can only launch one grid at a time. Blocks can be arranged in two dimensions, and the threads within the blocks can be arranged in three dimensions. Threads within blocks can cooperate via shared memory, but cannot cooperate with threads from another block. It is therefore pivotal for the performance of a parallel process that a domain analysis has been performed, and the memory intensive parts of the problem are contained within one block.

Blocks do not only have the role of sharing memory among threads. They also serve as a synchronization point for the threads within themselves. A special command within CUDA will make sure that all threads within a block have completed, before further execution is permitted.

To support this structure, CUDA provides built-in facilities for indexing blocks and threads. This allows the programmer to write code specifically for a single thread within a grid of many blocks, together containing millions of threads.

(TODO: make figure to show how grid and block structure can be in cuda)

\insfig{./images/cudaprocess.png}{0.5\textwidth}{CUDA process model (from Wikipedia article, author: Tosaka)}{cudaprocess}

\subsubsection{CUDA data model}

\insfig{./images/cudamem.png}{0.5\textwidth}{CUDA(GPU) memory model (TODO: where did I get this from?!)}{cudamem}
