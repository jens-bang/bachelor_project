\section{Theory}

\subsection{Lattice Boltzmann Model (\textit{Jens Bang})}
The Lattice Boltzmann Model (LBM) is a simplified version of the Boltzmann equation, which simulates the behaviour of fluid flows. The simplification consists of limiting the particles to only occupy certain points in space (vertices in a lattice), and to only travel along specified directional vectors, with constant speed. 

In this way the Lattice Boltzmann Model (LBM) describes simple fluids (gases and liquids), i.e. it ignores thermal effects and tracers. The LBM simulates movements in fluids by looking at particles found at points in a lattice at discrete time-steps. There are different versions of the LBM using either 2-dimensional or 3-dimensional lattices.

Each point in the lattice has a set of state-variables attached, describing the state of the particle found at that point. Each point also has a set of fixed directions, along which the particle can travel. In a 2-dimensional lattice there are normally 9 directions, while in a 3-dimensional lattice you see either 15 or 19 directions. These 3 setups are normally referred to as D2Q9, D3Q15 and D3Q19.

\insfig{./images/d2q9_d3q19.png}{0.5\textwidth}{LBM directions (from \cite{scholarpedialbm})}{lbmdirections}

The LBM divides the flow of gases or fluids into small, discrete time-steps. For each time-step the algorithm traverses all lattice points and for each point calculates the velocity and direction the particle travels in, as well as handling any collisions between particles.

\subsubsection{Using the LBM}\label{usingthelbm}
Since the LBM is especially well suited to simulate flows around even very complex geometric structures, it has a wide variety of practical uses. Among these are:
\begin{itemize}
\item Simulating pore-scale processes in porous media
\item Simulating Wind turbines
\item Simulating the water flow around bridge pillars
\end{itemize}

\subsection{Parallel processing (\textit{Jens Bang})}

Parallel computing is the execution of multiple processes on more than one CPU or processor core. Years ago computers with multiple CPUs or processor cores were rare, and most computers only had one CPU with one processor core. This meant that they had to simulate parallel processing, by using concurrency, also known as time slicing. Nowadays computers with multiple CPUs, or at least one multi-core CPU, are commonplace, so it is true parallel processing, where multiple tasks are physically run at the same time, each on their own CPU or processor core, or even each on their own separate computer.

Parallel processing can take on different guises: Sometimes the processes run the same code on different data, sometimes the processes run completely different code sets. An example of the latter is a program that uses one thread to receive a video stream from an IP camera, while another thread displays the received video images on the computer's screen. A well known example of the former, on a grand scale, is the SETI@Home program, where the immense amounts of data collected by SETI is broken up into small packets, which are then handed over to different private PCs to be processed individually by running the exact same code on each computer, only on different parts of the data set.

Sometimes parallel processing is performed by processes each working on their own discrete data set, as seen in SETI@Home, while at other times the processes work on a larger, shared data set. The important thing is not if they perform the same task or different tasks, nor is it important if the processes work on the same larger data set or on smaller, different data sets. The important thing in parallel processing is that the processes physically run at the same time on different CPUs or processor cores.

\subsection{CPU vs. GPU (\textit{Jens Bang})}
When comparing a CPU with a GPU, it is clear that different goals for processor functionality produce quite different designs. While the CPU has to be good for solving \emph{any} problem, parallel workload or not, the GPU is designed with a specific task in mind, a task with a highly parallel workload.

\insfig{./images/cpu-vs-gpu.png}{0.7\textwidth}{Visualization of the amount of a CPU is devoted to data processing as compared to a GPU (inspired by \cite{bestpracticesguide} figure 1-2 p. 3).}{cpuvsgpu}

Today's CPUs very often have more than one core, normally 2 or 4, which does let the CPU achieve a certain degree of parallelism. But because of the very often non-parallel nature of their workload, the CPU comes with a very sophisticated control logic, which is put in place to minimize the latency stemming from trying to parallelise a non-parallel workload. Today's CPUs also come with a big on-chip cache to minimize latency stemming from having to wait for the comparatively slow reads from general memory. All this results in a large part of the CPU being used for other tasks than the actual data processing.

Todays GPUs take advantage of the fact that graphics processing tasks are highly parallel, this is also known as data level parallelism, because the parallelism is built in on the data level so to speak. This fact helps the GPU to cut down on the control logic as compared to CPUs, since many of the non-parallel task problems seen in CPUs are simply not present in GPU tasks, and so the control logic to handle these problems are not needed in the GPU. This results in a maximized throughput of all processes in the GPU, since more of the GPU's computing power can be used for actual data processing.

The reason that data level parallelism does not need much control logic, is because the same set of instructions has to be performed on large parts of the data set at the same time, so the GPU can load a large block of data and then run the same instruction on each bite (this can be one byte or several, depending on the instruction) of data in unison. This form of parallelism is what Flynn calls SIMD (Single instruction, multiple data), and was used as early as the CM-1 from Thinking Machines Corporation.

The form of parallelism seen in nVidia's CUDA enabled GPUs is what nVidia call SIMT (Single Instruction Multiple Threads). The difference between original SIMD and SIMT is that under SIMT while each thread does run the same code, each individual thread doesn't have to execute exactly the same instruction at the same time. Threads can diverge, at which point the GPU's control logic will start cycling through sets of threads that agree on where in the executing code they are issuing the next instruction to each set of threads in order \cite[p.~81-82]{programmingguide}. 

\subsection{CUDA (\textit{Jacob Salomonsen})}
In the beginning, GPUs had only very limited purposes. Mostly they were about generating real-time graphics for games, but also in a few cases for production and scientific applications. This has changed since the proliferation of the pixel shader. A pixel shader basically produces a color for every point on a screen, by taking into account the (x,y) position of the pixel, the light settings of a scene, the material properties of objects in the scene and so on. Early researchers found that this ability to perform computations on each pixel could be harnessed to other uses. Since pixel shaders are completely controlled by the programmer, the possibility of simply giving a GPU data as input, instead of a scene to render, was within reach. This would mark the beginning of the general purpose GPU.

The general purpose GPU did however lack a platform for developers to build upon, since learning to program pixel shaders required previous knowledge about either OpenGL or DirectX. Also even when knowing these frameworks, the model would lack the perspective of general purpose computing and instead be focused on generating graphics, where the general purpose computing aspect would be achieved by "cheating" the GPU into treating data as if it was a scene to be rasterized.

Enter CUDA. CUDA is nVidia's way of introducing general purpose GPU programming to a wider audience. CUDA utilizes the parallel nature of the GPU, allowing a low end computer to process data in a different, and some times more efficient, way.

The CUDA API exposes a set of tags, which allows a programmer to easily specify what methods, in otherwise ordinary C code, should be executed on the GPU. The way CUDA makes it possible is by using a preprocessor which parses the source code and makes different files for the individual architectures. Thus the normal C code goes to whichever compiler the user chooses, and the parallel section of the code goes to a compiler specifically designed to generate PTX-code. PTX-code is essentially assembly which is not specific to any GPU. The PTX-code is then translated into assembly code specifically targeted at the GPU \cite{mpr}.

This effectively separates the tedium of writing assembly code directly aimed at a specific GPU-architecture, or writing pixel shaders via graphics APIs.

\subsubsection{CUDA programming model}
The CUDA programming model efficiently supports the steps needed to be taken, in order to make a process parallel. The model is arranged so a decomposition of the problem area is naturally ingrained in the programming procedure, by having the programmer split the problem into parts. Since the GPU is capable of launching many threads, it is necessary to have control of what thread is launched where. Therefore nVidia has structured execution of code in a way, that both allows an easy intuition of the GPU-architecture, as well as supporting the steps in making a process parallel.

The code to be executed on the GPU, \emph{the kernel}, launches a grid which contains blocks of threads. A kernel can only launch one grid at a time. Blocks can be arranged in up to two dimensions, and the threads within the blocks can be arranged in up to three dimensions. Threads within blocks can cooperate via shared memory, but cannot cooperate with threads from another block. It is therefore pivotal for the performance of a parallel process that a domain analysis has been performed, and the memory intensive parts of the problem are contained within one block.

Blocks do not only have the role of sharing memory among threads. They also serve as a synchronization point for the threads within themselves. A special command within CUDA will make sure that all threads within a block have completed, before further execution is permitted.

To support this structure, CUDA provides built-in facilities for indexing blocks and threads, listed in \autoref{cudaindex}. This allows the programmer to write code specifically for a single thread within a grid of many blocks, together containing millions of threads. The possible layouts of the grid structure can be seen in \autoref{gridblockthread}.

\begin{table}[htb]
	\centering
	\begin{tabular}{lll}
		\toprule
		Keyword & Dimensions & Usage \\
		\midrule
		threadIdx & x,y,z & index of the thread within the block \\
		blockIdx & x,y & index of the block  \\
		blockDim & x,y,z & amount of threads in each dimension \\
		gridDim & x,y & amount of blocks i each dimension \\
		\bottomrule
	\end{tabular}
	\caption{Indexing facilities available in the CUDA api}
	\label{cudaindex}
\end{table}


\newpage
\insfig{./images/gridblockthread.pdf}{0.60\textwidth}{\textbf{Top:} Two dimensional block layout. \textbf{Bottom:} Three dimensional block layout. When threads increase in a block the layout is under the forced maximum amount of threads per block, here assumed to be 256 threads (inspired by \cite{cudabyexample} figure 5.2 p. 71).}{gridblockthread}

When designing CUDA-programs it is important to split the problem up into parts that map directly into this block structure, so each thread has it's own specific and simple task.


\subsubsection{CUDA data model}
The process of working with CUDA consists of letting the CPU instruct the GPU what to do. In this is also contained the job of memory handling between the host (CPU) and the device (GPU). The CPU is therefore responsible for copying necessary data to the GPU for processing. This work-flow is illustrated in \autoref{cudaprocess}

\insfig{./images/cudaprocess.png}{0.5\textwidth}{CUDA process model (from Wikipedia article, author: Tosaka)}{cudaprocess}

CUDA exposes more than just one memory model to the programmer, each with a different purpose and performance. \autoref{cudamem} shows the different types of memory. In \autoref{cudamemtable} the different memory types are listed.

\begin{table}[htb]
	\centering
	\begin{tabular}{llll}
		\toprule
		Name 	& Speed & Access & Scope\\
		\midrule
		Global 	& 200-300 clock cycles 	& Read/write & All threads + host \\
		Texture 	& < 100 clock cycles & Read & All threads + host\\
		Shared 	& 1-2 clock cycles & Read/write & All threads in a block\\
		\bottomrule
	\end{tabular}
	\caption{A subset of the memory types the CUDA api exposes \cite{drdobbs}}
	\label{cudamemtable}
\end{table}

\insfig{./images/cudamem.png}{0.5\textwidth}{CUDA(GPU) memory model (from \cite{cudamem})}{cudamem}
