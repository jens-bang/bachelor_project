\section{Implementation (\textit{Jacob Salomonsen})}


\subsection{Matlab implementation analysis}
The NumPy and CUDA implementations take origin in a Matlab script. Since the project relies on a blackbox implementation scheme, it was necessary to first perform an analysis of the supplied script. The first step taken in the analysis was to map what each of the key variables purpose were. The result can be seen in \hpref{table}{matlabvars}

\begin{table}[htb]
	\centering
	\begin{tabular}{llp{6cm}}
		\toprule
		Variable & Type & Purpose \\
		\midrule
		F 				& $nx\times ny\times 9$ (double)	& eight directional matrices and one stationary state matrix\\
		FEQ 			& $nx\times ny\times 9$ (double)	& equilibrium state of the nine matrices\\

		BOUND 			& $nx\times ny$ (binary) 		& boundary defined by 1's ($b$)\\
		CI				& $8\times 1$ (integer) 			& linear index of directional matrices\\
		ON				& $b\times 1$ (integer)			& linear index of obstacle nodes\\
		TO\_REFLECT 		& $b\times 8$ (integer)			& linear index of nodes to save to BOUNCEBACK over all nine matrices\\
		REFLECTED 		& $b\times 8$ (integer) 			& linear index of nodes to recover from BOUNCEBACK over all nine matrices \\
		BOUNCEBACK		& $b\times 8$ (integer) 			& densities bouncing back\\
		DENSITY			& $nx\times ny$ (double) 		& summation of all nine matrices\\
		UX				& $nx\times ny$ (double) 		& summation of all densities with non zero x-component\\
		UY				& $nx\times ny$ (double) 		& summation of all densities with non zero y-component \\
		U\_SQU			& $nx\times ny$ (double) 		& sum of UX squared and UY squared\\
		U\_C2			& $nx\times ny$ (double) 		& sum of UX and UY\\
		U\_C4			& $nx\times ny$ (double) 		& negative sum of UX and UY\\
		U\_C6			& $nx\times ny$ (double) 		& negative of U\_C2\\
		U\_C8			& $nx\times ny$ (double) 		& negative of U\_C4\\
		\bottomrule
	\end{tabular}
	\caption{Variables and their purpose in the Matlab version of D2Q9 LBM}
	\label{matlabvars}
\end{table}

The main and most important variable in the script is the F-variable. From \hpref{table}{matlabvars} we see that F is a $nx\times ny\times 9$ matrix. This means that it has the size of the simulated field, and nine states of the the simulated fields. Hence according to the theory (TODO: reference to theory section) this variable stores eight density directions and one stationary state. 

The secondary result of the variable analysis was a clearer understanding of the procedures taking place upon running the Matlab script. The script can be divided into four separate parts: \textbf{Propagation}, \textbf{Density}, \textbf{Equilibrium}, \textbf{Bounceback}.

\subparagraph{Propagation} 
Propagation is where nodes densities are spread outward to affect neighbouring nodes. In the Matlab code this happens in line 44-55 in \hpref{listing}{lbmmatlab}. Lets examine line 45 and see how this is done

\begin{verbatim}
>> F(:,:,1)=F([nx 1:nx-1],:,1);
\end{verbatim}

On the right hand side a new matrix is created from F, by simply re-indexing the original array. Basically if the simulated field is $nx=10$ wide the statement \texttt{[nx 1:nx-1]} creates the following

\begin{verbatim}
10     1     2     3     4     5     6     7     8     9
\end{verbatim}

essentially propagating the data in an eastward direction, by wrapping the end of the matrix around and shifting forward the rest, corresponding to vector c1 in (TODO: reference to neat figure here). The rest of the expression says to go along all elements of the second axis, and only operate on matrix one out of nine. One thing to keep in mind when analysing Matlab code is that it uses one-based indexing, as opposed to most programming languages (e.g. Python) which uses zero-based indexing.

The rest of the propagation part of the script is simply an extension of the above, with only difference being the direction of the propagation, by shifting the indices appropriate to the matrix in question. (TODO: maybe reference theory for clarification of what directions there are?)

\subparagraph*{Density}
The first task done in the density part (line 57-61 in \hpref{listing}{lbmmatlab} of the Matlab script is the saving of densities bouncing back from obstacles. The way this is done is a little involved so it deserves some attention.

On line 21 of \hpref{listing}{lbmmatlab} the variable CI is generated, and as stated in \hpref{table}{matlabvars} the variable is the linear index of the first element of the 1--8 matrices (matrix 9 is not used for bounce back). This means that if the simulated field is $10\times10$ the indices would be

\begin{verbatim}
0   100   200   300   400   500   600   700
\end{verbatim}

which can be used together with the variable ON, which contains the linear index for obstacle nodes, to calculate the linear position of obstacle nodes in all of the 1--8 matrices (TODO: think up a way to illustrate this with some math...).

The indices of obstacles in matrix 1--8 is saved in the variable TO\_REFLECT and then used to access all positions within the F-variable where there exists an obstacle, bearing in mind that this is the same position per obstacle for each 1--8 matrix, just with different linear indices.

The density is easily calculated by use of the sum command, which sums the matrices up along the third axis, i.e. the density matrices 1--9 are reduced to one.

The UX and UY variables are created summing only the density matrices having either non-zero x- or y-component. The meaning of x- and y-component is illustrated in (TODO:insert snazzy figure illustrating the directional vectors summed across axes).

\subparagraph*{Equilibrium}
The equilibrium part that runs from 63--88 increases the inlet pressure as to create a constant flow in the simulation. This part also uses the result from the density part to calculate some constants used in calculation of the equilibrium. The equilibrium is calculated for each density matrix 1--9, the first one being the stationary state as seen below.

\begin{verbatim}
FEQ(:,:,9)=t1*DENSITY.*(1-U_SQU/(2*c_squ));
\end{verbatim}

Here the special '.' operator is used, meaning that it is an element-wise operation, not on the full matrix.

Subsequently the FEQ-variable is added to the F-variable

\subparagraph*{Bounceback}
The final part bounce back is done just like at our first encounter with bounce back, only this time the directions have been shifted. The REFLECTED variable is just like TO\_REFLECT, only indices for density matrix 1 has been swapped with indices for density matrix 5, which is the opposite westward direction as seen on (TODO: snazzy figure...).



\subsection{NumPy}
NumPy is a scientific computing package for Python, which allows for multidimensional arrays, and routines for fast operations on these arrays. NumPy was used in implementing the CPU-version of the D2Q9-LBM. NumPy has the disadvantage that it will only run on one CPU, even in a multicore processor (TODO: add reference for this). This severely limits the potential for scientific computing using this package, but the usefulness of this package is that it is easy to prototype calculations.

At the core of the NumPy package is the \textit{ndarray}. The \textit{ndarray} is a statically allocated data structure, which upon creating cannot change in size. If an attempt to change the size of it is made it is simply overwritten. To allocate an array fitting with the data layout discussed in an earlier section (TODO: perhaps reference to the section here?), the following command is used:

\begin{verbatim}
>>> F = numpy.zeros((9,nx,ny), dtype=float)
\end{verbatim}

This allocates a three dimensional array consisting of zeros. Here the first dimension contains 9 spaces, and the other two dimensions contain the given width and height of the simulated field. The \texttt{dtype} parameter decides what data-type is to be used, in this case a float of no specification. 

As an alternative an array can be allocated with ones instead of zeros, which is useful for generating the scenery in the simulation field. This is simply done by the command

\begin{verbatim}
>>> BOUNDi = numpy.ones(BOUND.shape, dtype=float)
\end{verbatim}

Arrays have certain attributes attached to then which can be accessed by the dot operator. In the command above it can be seen that one of these attributes is accessed in the context of creating a new array, to make that array the same shape as another array. Here the meaning of the command is to create an array consisting of ones called \texttt{BOUNDi} in the shape of \texttt{BOUND}.

The type of operation which is mostly used in the LBM is the slicing operation. In the propagation kernel the data of the arrays need to "propagate" in specific directions. The slicing is used to move parts of the array around by assigning a shifted slice to the original matrix, while wrapping the end around to the beginning of the array. This of course requires careful consideration so as to avoid writing already shifted data which would lead to undesired results. This problem is easily solved by having a temporary store for the matrix and shifting the temporary matrix. The mechanics of this procedure is illustrated like this

\begin{tabular}{|c|c|c|}
\hline
1 & 2 & 3 \\
\hline
4 & 5 & 6 \\
\hline
7 & 8 & 9 \\
\hline
\end{tabular}

When dealing with data in an euclidean space it is important to remember that the array class in NumPy is column major. Therefore methods for example used for plotting data will expect the first dimension of a two-dimensional array to correspond with the y-axis of a normal plot. 

NumPy supports summation operations so as to be able to reduce the nine directional matrices. This is done by the use of the command

\begin{verbatim}
>>> DENSITY = numpy.add.reduce(F)
\end{verbatim}

Which produces the result of adding all the matrices along the first dimension, being the nine separate directional matrices. This could of course have been done even if the first dimension was not the directional matrices but instead the x-axis of the simulated field. Then an argument would have to be given to the \texttt{add.reduce} command to tell it along which axis to sum.

\begin{verbatim}
>>> DENSITY = numpy.add.reduce(F, axis=2)
\end{verbatim}

To expand a matrix with a new dimension, which is needed for the boundary calculations. Since the boundary matrix is only two dimensional it is necessary to expand upon it to be able to impose the boundary on the directional matrices. This is done by the command

\begin{verbatim}
>>> BOUND[numpy.newaxis,:,:]
\end{verbatim}



\subsection{PyCUDA}
PyCUDA is an extension for the Python language, that allows for full access to functionality available to CUDA C. Since Python is a scripting language, which means it is not compiled but interpreted, the programming process is leveraged. One notable advantage over CUDA C is automatic resource control. Another advantage is the tight coupling with NumPy, which is especially good for the main purpose of this report. The testing procedure will also gain more reliability by keeping the execution on the same platform.

PyCUDA allows a programmer trained in the usage of CUDA C, to easily start programming. Basically PyCUDA allows for CUDA C-code to be entered directly into the Python script. This is afforded by the \texttt{SourceModule}, which is a command used for creating CUDA-kernels. The \texttt{SourceModule} utilizes a just-in-time compilation process. (TODO: check fact!) This means that the kernel is only compiled at the moment it is needed.

As with CUDA C it is also necessary to load data onto the GPU by first allocating space and then copying to the GPU-memory. PyCUDA allows for allocating memory on the GPU using the \texttt{driver.mem\_ alloc} command, with argument for the size of the memory to allocate. This command will allocate global memory on the GPU, which is the memory both the CPU and GPU can read and write to (TODO: check whether the copy is just for a pointer to the python buffer or if data is loaded into gpu-memory). Copying to the GPU is performed with the \texttt{device.memcpy\_ htod}. Copying back from the GPU is afforded by the command \texttt{driver.memcpy\_ dtoh}.

When creating variables in NumPy, meant to be transfered to the GPU by PyCUDA, it is important to specify a data-type the GPU will accept. For example to specify what kind of float type a new array should be created with, the command from the NumPy section can be augmented with the following statement

\begin{verbatim}
>>> F = numpy.zeros((9,nx,ny), dtype=float).astype(numpy.float32)
\end{verbatim}

The \texttt{astype} ensures data is allocated as 32-bit floats, which are especially suited for CUDA.

PyCUDA allows for allocation to different memory types on the GPU as seen in \hpref{figure}{cudamem}. We have already covered global memory. What remains is one of the CPU read/write GPU read memories, the texture-memory. The texture-memory is a special kind of memory which exhibits a spatial characteristic. This means that data most suited for this memory is either two or three dimensional. This memory is relatively fast compared with the global memory, but the drawback is it is read only. The allocation and usage is reminiscent of the CUDA C method. As with CUDA C a texture is defined in the kernel, which is contained withing PyCUDA's \texttt{SourceModule}

\begin{verbatim}
texture<float, 2> tex;
\end{verbatim}

Now as with CUDA C the texture reference is tied to the texture's 'name' in the kernel

\begin{verbatim}
texture_func = mod.get_function("kernel_handle")
texref = mod.get_texref("tex")
\end{verbatim}

Now the that the reference has been set up, it is possible to load data into the texture. Loading a matrix into the texture is done so

\begin{verbatim}
>>> pycuda.driver.matrix_to_texref(a, texref, order="C")
\end{verbatim}

Here the \texttt{a} is used for the kernel call, and exists on the GPU-side. The last parameter decides what order the dimensions are, if it is 'C' then \texttt{tex2D(x,y)} is going to fetch \texttt{matrix(y,x)}, otherwise if 'F' \texttt{tex2D(x,y)} is going to fetch \texttt{matrix(x,y)} (TODO: cite the pycuda manual).

A very neat feature about PyCUDA is the GPUArray class. The GPUArray behaves like the \textit{ndarray}, but performs its computations on the GPU instead of one CPU-core. To create an empty GPUArray with an \textit{ndarray} as template the following command can be used.

\begin{verbatim}
a = gpuarray.empty_like(x)
\end{verbatim}

To 

\begin{verbatim}
a = gpuarray.to_gpu(numpy.array(5, dtype=numpy.float32))
\end{verbatim}

Hence it the memory allocation and copying to memory is mostly abstracted away.

A GPUArray can also bind to a texture reference.

The GPUArray can perform basic linear algebra. 




\newpage
\subsection{Design choices}
In this section the choices made during the implementation of the D2Q9 LBM will described. The implementation takes origin in the Matlab code included in \hpref{listing}{lbmmatlab}.

To be able to perform a proper comparison between the CPU-- and GPU--version of the model, a NumPy version has been created also taking origin in the Matlab code. This allows for the same benchmarking technique within the Python code, using the (TODO: insert library name here).

\subsubsection{Data layout}
The D2Q9 LBM maintains velocities for eight directional states plus one stationary state per point in the simulated field. If the dimension of the simulated field is $x \cdot y$ then to be able to contain this, the data structure must be able to contain $9 \cdot x \cdot y$.

\subsubsection{NumPy}
Since Python (and NumPy) is reminiscent of Matlab, there are no significant differences between the Matlab-- and NumPy--version. The largest difference lies in the above method for wrapping an array and the way the boundary is imposed on the directional matrices. This is done by using two matrices, one normal and one inverted matrix containing zeros for the locations of boundaries and ones for the locations of free space. The normal matrix is used for selecting only the nodes in the boundary so as to save their propagational state for later use in the bounceback section. The inverted boundary matrix is then multiplied on the eight directional matrices which results in an erasure of all other than the boundary nodes' stationary matrix. Using the saved state from the boundary nodes velocities are then transferred back in an inverted pattern, as described in the (TODO: refer to code analysis section).

\subsubsection{PyCUDA}
The implementation of the PyCUDA version takes origin in the previous analysis of the Matlab-script. The analysis yielded a clear picture of the procedures inherent in the program. This is key knowledge in attempting to parallelise an algorithm.

The knowledge gained from the analysis showed that the procedures in the Matlab-script exhibited characteristics necessary for a program to become parallel. The problem is very easily decomposed to much smaller sub problems. The object from then on was to think out a proper way to arrange data and thread execution. In the end it was decided to arrange each lattice node, so that this would be processed by a single GPU-thread. This means that each thread is a single unit in the lattice, taking care of the same data-location in all nine matrices.

The main challenge in arranging data is that it is important each thread is treated as a single non-dependent unit, which on execution knows where it is and what data it is supposed to operate on. To help, CUDA supplies some indexing constants for use in the kernel as stipulated in the CUDA section (TODO: reference).

Since CUDA C does not have a construct like NumPy's \textit{ndarray}, it is necessary to linearise data access. Normally this would simply consist of assigning a maximum size of the array and let access to an element higher than that width wrap around and increase in the second dimension (TODO: illustrate this fact). This however is complicated by the fact that we do not run through the array accessing each element in sequence. Instead we access the array from certain points within the CUDA block structure. This meaning each thread has it's own specific data-point in the array. Thus we need to linearise the array with the tools given to us in the CUDA programming model.

This is done by clever use of the block index, block dimension and thread index. Imagine we have a linear array we need to spread out over a series of CUDA blocks. The kernel has been launched with a pre-calculated block size, and the amount of blocks have been calculated to fit with the data in question. If we take a practical example and say our blocks are one-dimensional, and they can as a maximum contain 10 threads. Your array contains 35 elements, and we want to be able to index this within the CUDA-kernel. The linear index mapping from thread location in the CUDA kernel to an array index is then calculated like so

\begin{verbatim}
idx = threadIdx.x + blockIdx.x * blockDim.x;
\end{verbatim}

in figure (TODO: make a figure like in notes), it can be seen how this works for two thread locations within the CUDA-kernel. The block dimension is used together with the block index, to determine what block the thread is within, then it is only to add the thread index within the found block and we have the linear index. This technique is used throughout the code in implementing the GPU-version of the D2Q9 LBM, only extended to either two or three dimensions.

Initially the project focused on implementing the D3Q19 LBM so some work went into arranging data to fit this model. As blocs are limited as to how many threads can run within them, the main issue which must be resolved when making an algorithm parallel arises. For the D3Q19 case the main issue was to fit a three dimensional problem into a two dimensional space. A block can contain three dimensions, but this will minimize the amount of threads available to run in each dimension. Also, the third dimension will still be limited to the maximum allowable threads per block(TODO: insert a figure illustrating this limitation). The solution is to linearise the third dimension into the grid layout, where a block can have two dimensions corresponding to the simulated field's size, and then be repeated as a series of blocks in the grid corresponding to the third dimension.

(TODO: insert figure of three dimensional layout)

This however raises another problem. What if the simulated field is larger than the allowable size of the block? When the simulated field crosses the maximum block size, four blocks is needed to support this increase (assuming the simulated field is always quadratic). Since the grid can only be two dimensional there is a difficult problem in linearising the blocks from three to two dimensions.

The two dimensional case D2Q9 is however much simpler. Here it is just a matter of extending the one dimensional case illustrated above

\begin{verbatim}
int x     = threadIdx.x + blockIdx.x * blockDim.x;
int y     = threadIdx.y + blockIdx.y * blockDim.y;
int nx    = blockDim.x * gridDim.x;
int cur = x + y * nx;
\end{verbatim}

This successfully emulates a two dimensional array by using the block and thread index in the y-direction also. The trick is that when the y-coordinate increases the index has to follow the principle of the linearisation. Here when a y-coordinate increases it really means it has surpassed a certain amount of horizontal strips in the array, where the length of these strips are added to the index (TODO: illustrate). So to clarify the code above, the linear index of the x-coordinate is calculated as before, as well as the y-index. But we cannot access the array with these two indices, since the array is one dimensional. Therefore the y-dimension must be extended along a linear axis. This is simply done by increasing the index by nx, which is the horizontal dimension of the simulated field (being a block's size times the amount of blocks) for every y-coordinate.

We are still missing one thing before we are ready with our indexing-scheme. We need to access nine separate matrices also by linearising their starting position, much like in the original Matlab-script's variable \texttt{CI}. As described in the data-structure section, index 0 has been chosen to contain the stationary matrix instead of 9 as in the Matlab-script. In this way all directional matrices can keep numbering consistently with the Matlab-script since Matlab uses one-based indexing and Python uses zero-based indexing.

To get the starting position of the other matrices, it is simply a matter of finding the size of the matrix, and adding to the index already found above

\begin{verbatim}
int nx    = blockDim.x * gridDim.x;
int ny    = blockDim.y * gridDim.y;
int size  = nx * ny;
\end{verbatim}



Now that the indexing problem is solved, the method can be used throughout the program since the same data-structure as described in the section (TODO: section reference) will be used for all kernels.




no convenience like slicing... linearization


