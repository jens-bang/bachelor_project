\section{Implementation (\textit{Jacob Salomonsen})}


\subsection{Matlab implementation analysis}
The NumPy and CUDA implementations take origin in a Matlab script. The analysis has been made on the D2Q9 version, but the D3Q19 is just an extension of this with en extra dimension. Since the project relies on a blackbox implementation scheme, it was necessary to first perform an analysis of the supplied script. The first step taken in the analysis was to map what each of the key variables purpose were. The result can be seen in \hpref{table}{matlabvars}

\begin{table}[htb]
	\centering
	\begin{tabular}{llp{6cm}}
		\toprule
		Variable & Type & Purpose \\
		\midrule
		F 				& $nx\times ny\times 9$ (double)	& eight directional matrices and one stationary state matrix\\
		FEQ 			& $nx\times ny\times 9$ (double)	& equilibrium state of the nine matrices\\

		BOUND 			& $nx\times ny$ (binary) 		& boundary defined by 1's ($b$)\\
		CI				& $8\times 1$ (integer) 			& linear index of directional matrices\\
		ON				& $b\times 1$ (integer)			& linear index of obstacle nodes\\
		TO\_REFLECT 		& $b\times 8$ (integer)			& linear index of nodes to save to BOUNCEBACK over all nine matrices\\
		REFLECTED 		& $b\times 8$ (integer) 			& linear index of nodes to recover from BOUNCEBACK over all nine matrices \\
		BOUNCEBACK		& $b\times 8$ (integer) 			& densities bouncing back\\
		DENSITY			& $nx\times ny$ (double) 		& summation of all nine matrices\\
		UX				& $nx\times ny$ (double) 		& summation of all densities with non zero x-component\\
		UY				& $nx\times ny$ (double) 		& summation of all densities with non zero y-component \\
		U\_SQU			& $nx\times ny$ (double) 		& sum of UX squared and UY squared\\
		U\_C2			& $nx\times ny$ (double) 		& sum of UX and UY\\
		U\_C4			& $nx\times ny$ (double) 		& negative sum of UX and UY\\
		U\_C6			& $nx\times ny$ (double) 		& negative of U\_C2\\
		U\_C8			& $nx\times ny$ (double) 		& negative of U\_C4\\
		\bottomrule
	\end{tabular}
	\caption{Variables and their purpose in the Matlab version of D2Q9 LBM}
	\label{matlabvars}
\end{table}

The main and most important variable in the script is the F-variable. From \hpref{table}{matlabvars} we see that F is a $nx\times ny\times 9$ matrix. This means that it has the size of the simulated field, and nine states of the the simulated fields. Hence according to the theory (TODO: reference to theory section) this variable stores eight density directions and one stationary state. 

The secondary result of the variable analysis was a clearer understanding of the procedures taking place upon running the Matlab script. The script can be divided into four separate parts: \textbf{Propagation}, \textbf{Density}, \textbf{Equilibrium}, \textbf{Bounceback}.

\subparagraph{Propagation} 
Propagation is where nodes densities are spread outward to affect neighbouring nodes. In the Matlab code this happens in line 44-55 in \hpref{listing}{lbmmatlab}. Lets examine line 45 and see how this is done

\begin{verbatim}
>> F(:,:,1)=F([nx 1:nx-1],:,1);
\end{verbatim}

On the right hand side a new matrix is created from F, by simply re-indexing the original array. Basically if the simulated field is $nx=10$ wide the statement \texttt{[nx 1:nx-1]} creates the following

\begin{verbatim}
10     1     2     3     4     5     6     7     8     9
\end{verbatim}

essentially propagating the data in an eastward direction, by wrapping the end of the matrix around and shifting forward the rest, corresponding to vector c1 in \hpref{figure}{lbmdirections}. The rest of the expression says to go along all elements of the second axis, and only operate on matrix one out of nine. One thing to keep in mind when analysing Matlab code is that it uses one-based indexing, as opposed to most programming languages (e.g. Python) which uses zero-based indexing.

The rest of the propagation part of the script is simply an extension of the above, with only difference being the direction of the propagation, by shifting the indices appropriate to the matrix in question. (TODO: maybe reference theory for clarification of what directions there are?)

\subparagraph*{Density}
The first task done in the density part (line 57-61 in \hpref{listing}{lbmmatlab} of the Matlab script is the saving of densities bouncing back from obstacles. The way this is done is a little involved so it deserves some attention.

On line 21 of \hpref{listing}{lbmmatlab} the variable CI is generated, and as stated in \hpref{table}{matlabvars} the variable is the linear index of the first element of the 1--8 matrices (matrix 9 is not used for bounce back). This means that if the simulated field is $10\times10$ the indices would be

\begin{verbatim}
0   100   200   300   400   500   600   700
\end{verbatim}

which can be used together with the variable ON, which contains the linear index for obstacle nodes, to calculate the linear position of obstacle nodes in all of the 1--8 matrices (TODO: think up a way to illustrate this with some math...).

The indices of obstacles in matrix 1--8 is saved in the variable TO\_REFLECT and then used to access all positions within the F-variable where there exists an obstacle, bearing in mind that this is the same position per obstacle for each 1--8 matrix, just with different linear indices.

The density is easily calculated by use of the sum command, which sums the matrices up along the third axis, i.e. the density matrices 1--9 are reduced to one.

The UX and UY variables are created summing only the density matrices having either non-zero x- or y-component. The meaning of x- and y-component is illustrated in \hpref{figure}{uxuy}.

\insfig{./images/uxuy.eps}{0.7\textwidth}{The \texttt{UX} and \texttt{UY} variables are created}{uxuy}

\subparagraph*{Equilibrium}
The equilibrium part that runs from 63--88 increases the inlet pressure as to create a constant flow in the simulation. This part also uses the result from the density part to calculate some constants used in calculation of the equilibrium. The equilibrium is calculated for each density matrix 1--9, the first one being the stationary state as seen below.

\begin{verbatim}
FEQ(:,:,9)=t1*DENSITY.*(1-U_SQU/(2*c_squ));
\end{verbatim}

Here the special '.' operator is used, meaning that it is an element-wise operation, not on the full matrix.

Subsequently the FEQ-variable is added to the F-variable

\subparagraph*{Bounceback}
The final part bounce back is done just like at our first encounter with bounce back, only this time the directions have been shifted. The REFLECTED variable is just like TO\_REFLECT, only indices for density matrix 1 has been swapped with indices for density matrix 5, which is the opposite westward direction as seen on (TODO: snazzy figure...).



\subsection{NumPy}
NumPy is a scientific computing package for Python, which allows for multidimensional arrays, and routines for fast operations on these arrays. NumPy was used in implementing the CPU-version of the D2Q9-LBM. NumPy has the disadvantage that it will only run on one CPU, even in a multicore processor (TODO: add reference for this). This severely limits the potential for scientific computing using this package, but the usefulness of this package is that it is easy to prototype calculations.

At the core of the NumPy package is the \textit{ndarray}. The \textit{ndarray} is a statically allocated data structure, which upon creating cannot change in size. If an attempt to change the size of it is made it is simply overwritten. To allocate an array fitting with the data layout discussed in an earlier section (TODO: perhaps reference to the section here?), the following command is used:

\begin{verbatim}
>>> F = numpy.zeros((9,nx,ny), dtype=float)
\end{verbatim}

This allocates a three dimensional array consisting of zeros. Here the first dimension contains 9 spaces, and the other two dimensions contain the given width and height of the simulated field. The \texttt{dtype} parameter decides what data-type is to be used, in this case a float of no specification. 

As an alternative an array can be allocated with ones instead of zeros, which is useful for generating the scenery in the simulation field. This is simply done by the command

\begin{verbatim}
>>> BOUNDi = numpy.ones(BOUND.shape, dtype=float)
\end{verbatim}

Arrays have certain attributes attached to then which can be accessed by the dot operator. In the command above it can be seen that one of these attributes is accessed in the context of creating a new array, to make that array the same shape as another array. Here the meaning of the command is to create an array consisting of ones called \texttt{BOUNDi} in the shape of \texttt{BOUND}.

When dealing with data in an euclidean space it is important to remember that the array class in NumPy is column major. Therefore methods for example used for plotting data will expect the first dimension of a two-dimensional array to correspond with the y-axis of a normal plot. 

NumPy supports summation operations so as to be able to reduce the nine directional matrices. This is done by the use of the command

\begin{verbatim}
>>> DENSITY = numpy.add.reduce(F)
\end{verbatim}

Which produces the result of adding all the matrices along the first dimension, being the nine separate directional matrices. This could of course have been done even if the first dimension was not the directional matrices but instead the x-axis of the simulated field. Then an argument would have to be given to the \texttt{add.reduce} command to tell it along which axis to sum.

\begin{verbatim}
>>> DENSITY = numpy.add.reduce(F, axis=2)
\end{verbatim}

To expand a matrix with a new dimension, which is needed for the boundary calculations. Since the boundary matrix is only two dimensional it is necessary to expand upon it to be able to impose the boundary on the directional matrices. This is done by the command

\begin{verbatim}
>>> BOUND[numpy.newaxis,:,:]
\end{verbatim}



\subsection{PyCUDA}
PyCUDA is an extension for the Python language, that allows for full access to functionality available to CUDA C. Since Python is a scripting language, which means it is not compiled but interpreted, the programming process is leveraged. The workflow of PyCUDA is shown in \hpref{figure}{pcwork}. One notable advantage over CUDA C is automatic resource control. Another advantage is the tight coupling with NumPy, which is especially good for the main purpose of this report. The testing procedure will also gain more reliability by keeping the execution on the same platform.

\insfig{./images/pycudaworkflow.pdf}{1.0\textwidth}{PyCUDA workflow (from \cite{kloeckner_pycuda_2009})}{pcwork}

PyCUDA allows a programmer trained in the usage of CUDA C, to easily start programming. Basically PyCUDA allows for CUDA C-code to be entered directly into the Python script. This is afforded by the \texttt{SourceModule}, which is a command used for creating CUDA-kernels. A kernel is defined as a string like this

\begin{verbatim}
mod = SourceModule("""
__global__ void multiply_them(float *dest, float *a, float *b)
{
  const int i = threadIdx.x;
  dest[i] = a[i] * b[i];
}
""")
\end{verbatim}

This is just ordinary CUDA C, embedded in the Python code. To get the handle for the kernel this is done

\begin{verbatim}
multiply_them = mod.get_function("multiply_them")
\end{verbatim}

Now \texttt{multiply\_them} can be called and the kernel will run with supplied arguments. When passing parameters as constant, it is easy to string replace them into the kernel before it gets turned into a CUDA kernel. This is done like so

\begin{verbatim}
mult_string = """
__global__ void multiply_them(float *dest, float *a, float *b)
{
  const int i = threadIdx.x;
  dest[i] = a[i] * b[i] * %s;
}
"""
mult_string = mult_string % constant
mod = SourceModule(mult_string)
\end{verbatim}

Here the \%s denotes the location for replacement and the string is inserted right after the kernel string. As usual the string is entered into the \texttt{SourceModule} and the kernel is ready to go.

As with CUDA C it is also necessary to load data onto the GPU by first allocating space and then copying to the GPU-memory. PyCUDA allows for allocating memory on the GPU using the \texttt{driver.mem\_alloc} command, with argument for the size of the memory to allocate. This command will allocate global memory on the GPU, which is the memory both the CPU and GPU can read and write to (TODO: check whether the copy is just for a pointer to the python buffer or if data is loaded into gpu-memory). Copying to the GPU is performed with the \texttt{device.memcpy\_htod}. Copying back from the GPU is afforded by the command \texttt{driver.memcpy\_dtoh}.

When creating variables in NumPy, meant to be transfered to the GPU by PyCUDA, it is important to specify a data-type the GPU will accept. For example to specify what kind of float type a new array should be created with, the command from the NumPy section can be augmented with the following statement

\begin{verbatim}
>>> F = numpy.zeros((9,nx,ny), dtype=float).astype(numpy.float32)
\end{verbatim}

The \texttt{astype} ensures data is allocated as 32-bit floats, which are especially suited for CUDA.

PyCUDA allows for allocation to different memory types on the GPU as seen in \hpref{figure}{cudamem}. We have already covered global memory. What remains is one of the CPU read/write GPU read memories, the texture-memory. The texture-memory is a special kind of memory which exhibits a spatial characteristic. This means that data most suited for this memory is either two or three dimensional. This memory is relatively fast compared with the global memory, but the drawback is it is read only. The allocation and usage is reminiscent of the CUDA C method. As with CUDA C a texture is defined in the kernel, which is contained withing PyCUDA's \texttt{SourceModule}

\begin{verbatim}
texture<float, 2> tex;
\end{verbatim}

Now as with CUDA C the texture reference is tied to the texture's 'name' in the kernel

\begin{verbatim}
texture_func = mod.get_function("kernel_handle")
texref = mod.get_texref("tex")
\end{verbatim}

Now the that the reference has been set up, it is possible to load data into the texture. Loading a matrix into the texture is done so

\begin{verbatim}
>>> pycuda.driver.matrix_to_texref(a, texref, order="C")
\end{verbatim}

Here the \texttt{a} is used for the kernel call, and exists on the GPU-side. The last parameter decides what order the dimensions are, if it is 'C' then \texttt{tex2D(x,y)} is going to fetch \texttt{matrix(y,x)}, otherwise if 'F' \texttt{tex2D(x,y)} is going to fetch \texttt{matrix(x,y)} (TODO: cite the pycuda manual).

A very neat feature about PyCUDA is the GPUArray class. The GPUArray behaves like the \textit{ndarray}, but performs its computations on the GPU instead of one CPU-core. To create an empty GPUArray with an \textit{ndarray} as template the following command can be used

\begin{verbatim}
a = gpuarray.empty_like(x)
\end{verbatim}

To copy an existing \textit{ndarray} to the GPU using GPUArray following can be used

\begin{verbatim}
a_gpu = gpuarray.to_gpu(numpy.array(5, dtype=numpy.float32))
\end{verbatim}

Hence it the memory allocation and copying to memory is mostly abstracted away and \texttt{a\_gpu} can be used as an argument for a kernel call immediately. To return the array to the host the following can be called

\begin{verbatim}
a_cpu = a_gpu.get()
\end{verbatim}

which will return\texttt{a\_gpu} to a normal \textit{ndarray} \texttt{a\_cpu}. What is special about the GPUArray is that it can perform basic linear algebra on the GPU without creating kernels for this. This adds to the convenience of easy memory allocation making it an attractive alternative. It is limited though. It is not compatible with custom elementwise operations needed in the program in question, therefore it was opted out.



\subsection{Design choices}
In this section the choices made during the implementation of the D2Q9 LBM will described. The implementation takes origin in the Matlab code included in \hpref{listing}{lbmmatlab}.

To be able to perform a proper comparison between the CPU-- and GPU--version of the model, a NumPy version has been created also taking origin in the Matlab code. This allows for the same benchmarking technique within the Python code, using the \texttt{timeit} class.



\subsubsection{Data layout}
The data layout is well founded by the analysis of the Matlab script to consist of a two dimensional matrix, repeated nine times for the D2Q9 LBM. The D2Q9 LBM maintains densities for eight directional states plus one stationary state per point in the simulated field. If the dimension of the simulated field is $nx \cdot ny$ then to be able to contain this, the data structure must be able to contain $9 \cdot nx \cdot ny$. For both the NumPy and PyCUDA version the matrix index has been chosen as the first dimension instead of the third as it is in the Matlab script. This is because it is easier to handle upon linearisation and will fit the usage of some commands like \texttt{add.reduce}.



\subsubsection{NumPy}
Since Python (and NumPy) is reminiscent of Matlab, there are no significant differences between the Matlab- and NumPy-version. The largest difference lies in the method for wrapping an array and the way the boundary is imposed on the directional matrices.

Wrapping is done in lines (TODO:refer to listing) and relies on matrix slicing. The slicing is used to move parts of the array around by assigning a shifted slice to the original matrix, while wrapping the end around to the beginning of the array. This of course requires careful consideration so as to avoid writing already shifted data which would lead to undesired results. This problem is easily solved by having a temporary store for the matrix and shifting the temporary matrix.

\begin{verbatim}
F[1,:,0]     = T[1,:,-1]
F[1,:,1:]    = T[1,:,:-1]
\end{verbatim}

The first assignment accesses a negative index in the temporary matrix, which in NumPy wraps around to the end of the matrix. This is assigned to the first element in the matrix. Subsequently elements one to the end (\texttt{1:}) are assigned to elements zero to next last (\texttt{:-1}). The mechanics of this procedure is illustrated in \hpref{figure}{numpywrap}

\insfig{./images/numpywrap.png}{0.7\textwidth}{Wrapping and shifting mechanism in one direction for NumPy}{numpywrap}

The boundary bounce back calculations done by using two matrices, one normal and one inverted matrix containing zeros for the locations of boundaries and ones for the locations of free space. The normal matrix is used for selecting only the nodes in the boundary so as to save their propagational state for later use in the bounceback section. The inverted boundary matrix is then multiplied on the eight directional matrices which results in an erasure of all other than the boundary nodes' stationary matrix. Using the saved state from the boundary nodes velocities are then transferred back in an inverted pattern, as described in the (TODO: refer to code analysis section).



\subsubsection{PyCUDA}
The implementation of the PyCUDA version takes origin in the previous analysis of the Matlab-script. The analysis yielded a clear picture of the procedures inherent in the program. This is key knowledge in attempting to parallelise an algorithm.

The knowledge gained from the analysis showed that the procedures in the Matlab-script exhibited characteristics necessary for a program to become parallel. The problem is very easily decomposed to much smaller sub problems. The object from then on was to think out a proper way to arrange data and thread execution. In the end it was decided to arrange each lattice node, so that this would be processed by a single GPU-thread. This means that each thread is a single unit in the lattice, taking care of the same data-location in all nine matrices.

The main challenge in arranging data is that it is important each thread is treated as a single non-dependent unit, which on execution knows where it is and what data it is supposed to operate on. To help, CUDA supplies some indexing constants for use in the kernel as stipulated in the CUDA section (TODO: reference).

Since CUDA C does not have a construct like NumPy's \textit{ndarray}, it is necessary to linearise data access. Normally this would simply consist of assigning a maximum size of the array and let access to an element higher than that width wrap around and increase in the second dimension (TODO: illustrate this fact). This however is complicated by the fact that we do not run through the array accessing each element in sequence. Instead we access the array from certain points within the CUDA block structure. This meaning each thread has it's own specific data-point in the array. Thus we need to linearise the array with the tools given to us in the CUDA programming model.

This is done by clever use of the block index, block dimension and thread index. Imagine we have a linear array we need to spread out over a series of CUDA blocks. The kernel has been launched with a pre-calculated block size, and the amount of blocks have been calculated to fit with the data in question. If we take a practical example and say our blocks are one-dimensional, and they can as a maximum contain 10 threads. Your array contains 35 elements, and we want to be able to index this within the CUDA-kernel. The linear index mapping from thread location in the CUDA kernel to an array index is then calculated like so

\begin{verbatim}
idx = threadIdx.x + blockIdx.x * blockDim.x;
\end{verbatim}

in \hpref{figure}{linearindex}, it can be seen how this works for two thread locations within the CUDA-kernel. The block dimension is used together with the block index, to determine what block the thread is within, then it is only to add the thread index within the found block and we have the linear index. This technique is used throughout the code in implementing the GPU-version of the D2Q9 LBM, only extended to either two or three dimensions.

\insfig{./images/linearindex.pdf}{1.0\textwidth}{Calculating the linear index of data-point in an array for the CUDA thread to access}{linearindex}

Initially the project focused on implementing the D3Q19 LBM so some work went into arranging data to fit this model. As blocs are limited as to how many threads can run within them, the main issue which must be resolved when making an algorithm parallel arises. For the D3Q19 case the main issue was to fit a three dimensional problem into a two dimensional space. A block can contain three dimensions, but this will minimize the amount of threads available to run in each dimension. Also, the third dimension will still be limited to the maximum allowable threads per block(TODO: insert a figure illustrating this limitation). The solution is to linearise the third dimension into the grid layout, where a block can have two dimensions corresponding to the simulated field's size, and then be repeated as a series of blocks in the grid corresponding to the third dimension.

(TODO: insert figure of three dimensional layout)

This however raises another problem. What if the simulated field is larger than the allowable size of the block? When the simulated field crosses the maximum block size, four blocks is needed to support this increase (assuming the simulated field is always quadratic). Since the grid can only be two dimensional there is a difficult problem in linearising the blocks from three to two dimensions.

The two dimensional case D2Q9 is however much simpler. Here it is just a matter of extending the one dimensional case illustrated above

\begin{verbatim}
int x     = threadIdx.x + blockIdx.x * blockDim.x;
int y     = threadIdx.y + blockIdx.y * blockDim.y;
int nx    = blockDim.x * gridDim.x;
int cur 	  = x + y * nx;
\end{verbatim}

This successfully emulates a two dimensional array by using the block and thread index in the y-direction also. The trick is that when the y-coordinate increases the index has to follow the principle of the linearisation. Here when a y-coordinate increases it really means it has surpassed a certain amount of horizontal strips in the array, where the length of these strips are added to the index (TODO: illustrate). So to clarify the code above, the linear index of the x-coordinate is calculated as before, as well as the y-index. But we cannot access the array with these two indices, since the array is one dimensional. Therefore the y-dimension must be extended along a linear axis. This is simply done by increasing the index by nx, which is the horizontal dimension of the simulated field (being a block's size times the amount of blocks) for every y-coordinate.

We are still missing one thing before we are ready with our indexing-scheme. We need to access nine separate matrices also by linearising their starting position, much like in the original Matlab-script's variable \texttt{CI}. As described in the data-structure section, index 0 has been chosen to contain the stationary matrix instead of 9 as in the Matlab-script. In this way all directional matrices can keep numbering consistently with the Matlab-script since Matlab uses one-based indexing and Python uses zero-based indexing.

To get the starting position of the other matrices, it is simply a matter of finding the size of the matrix, and adding to the index already found above

\begin{verbatim}
int nx    = blockDim.x * gridDim.x;
int ny    = blockDim.y * gridDim.y;
int size  = nx * ny;
\end{verbatim}

So when wanting to access the first matrix $1 \times \mathrm{\texttt{size}}$ is added to the index \texttt{cur}, the second matrix $2 \times \mathrm{\texttt{size}}$, and so on. Now that the indexing problem is solved, the method can be used throughout the program since the same data-structure as described in the section (TODO: section reference) will be used for all kernels.

When starting a kernel in CUDA, it needs to know the dimension of the blocks and how many blocks to launch in the grid. PyCUDA is of course no exception. The amount of blocks to launch is calculated after this simple formula

\begin{verbatim}
dim         = 16
blockDimX   = min(nx,dim)
blockDimY   = min(ny,dim)
gridDimX    = (nx+dim-1)/dim
gridDimY    = (ny+dim-1)/dim
\end{verbatim}

Here it is assumed that a block can maximally contain 256 threads ($16\times16$ threads). This might not be the case depending on the system, but it is selected as a lowest common denominator, and can at any time be changed to fit the system. First the block dimension is calculated. Since the simulated field can be smaller than the block size it is necessary to check for this. Once this is done the grid dimension can be calculated. This part relies on a trick commonly used in integer division (TODO: cite cuda by example). The result is the amount of blocks which are able to contain the requested amount of nodes. These parameters are given as arguments upon a kernel launch like this

\begin{verbatim}
prop(F_gpu, T_gpu, block=(blockDimX,blockDimY,1), grid=(gridDimX,gridDimY))
\end{verbatim}


\paragraph{Propagation kernel} The density kernel implementation can be seen at lines 103--143 of \hpref{listing}{lbmcuda}. In the Matlab-script, propagation relied on index shifting and recreation of arrays from this shifted index. In the NumPy code, propagation relied on array slicing and shifting by reassigning sliced parts to other indices than the original matrix. We do not have the luxury of these operations in CUDA, firstly because they are expensive, secondly because they do not fit with the parallel paradigm. We must instead think as a single unit in the matrix. What is it that is really happening when these re-indexing or shifting actions occur. Breaking it down we see that one node in the matrix actually transfers its value to the nearest neighbours around it.

(TODO: illustration of this single node propagation would be neat)

As mentioned before, threads must be aware of their position in data by inferring it from their position in the block/grid layout. This is already done above. What is missing is an indexing of the nearest neighbours, and the wrapping which occurs in the original code. Since we already have x- and y-index it is easy to get the nearest neighbours of a node, it is just a matter of adding and/or subtracting one to the x- and/or y-index, still keeping in mind that the y-index needs to be further linearised.

Wrapping the matrices is a bit more complex procedure. The thread has no concept of the matrix as is, therefore it is necessary that the thread knows what to do, if it's on the edge of the matrix and about to be propagated in a direction outside the matrix.

When a thread tries to access a space which is beyond the border of the matrix, the index should wrap around to the beginning of the matrix. This is usually done by using integer division. By calculating the index modulus (\%) total number of nodes we get this wrapping behaviour. If the matrix is 10 nodes wide for example and we want to propagate the last node in an eastward direction, we would have to wrap around and assign the value in the last node to the first node in the array. What happens is if we access element 11 we get

\begin{verbatim}
10 % 10 = 0
\end{verbatim}

keeping in mind that arrays are using zero-based index. This is exactly what we want to do and it will work for individual nodes independently, so they will know if they should wrap around depending on their location, and the direction of the wrapping in question. 

There is a snag in using this method though. CUDA does not behave well when a negative index is referenced, which is needed for five of the directions. Tests show that CUDA simply ignores the case

\begin{verbatim}
-1 % 10 = -1
\end{verbatim}

where the expected outcome would be

\begin{verbatim}
-1 % 10 = 9
\end{verbatim}

e.g. the last index in the array. Thus modulus cannot be used in this way. As a side note it is very inefficient to use integer division in CUDA, therefore it should be avoided altogether (TODO: cite cuda best practices here).

A better way to solve this problem relies on conditionals. Simply by letting the thread check if the node is on the edge of the matrix by comparing the current node's position with the maximum amount of nodes, or with zero if it is propagating the other direction sufficed. For brevity the ternary operator is used, which has the form

\begin{verbatim}
test ? true : false
\end{verbatim}

In this way it is easy to let each thread check if it is on the edge of the matrix and take the appropriate action if so, otherwise getting the index of its neighbour. This is done so

\begin{verbatim}
int F1 = (x==0?nx-1:x-1) + y * nx;
\end{verbatim}

This might seem reverse compared with previous code, since one would expect that the last index should be wrapped around, e.g. the node reaching the maximum of the matrix. This is not the case since we now view from a single node and need to accept densities from surrounding nodes. It becomes clearer when viewing the assignment

\begin{verbatim}
F[1*size + cur] = T[1*size + F1];
\end{verbatim}

Here the current node accepts density from the node in the calculated position, thus if the current node is at position zero, it should get density from position $nx-1$ in the array (the end of the array), thereby propagating density in an eastward fashion. The propagation of a single node is illustrated in \hpref{figure}{cudaprop}

\insfig{./images/cudaprop.pdf}{0.4\textwidth}{A single node accepting densities from surrounding neighbours}{cudaprop}

\paragraph{Density kernel}
The density kernel runs from 143--202 in \hpref{listing}{lbmcuda}. In the Matlab version the bounce back was handled by linearising the location of obstacle nodes throughout the nine matrices. This is not necessary in the CUDA since the data access is already linearised. It means that the code cleans up very nicely and becomes very efficient compared with both the Matlab and the NumPy version. To save the density of the obstacle nodes it is just a matter of sending the \texttt{BOUND} variable to the GPU, and testing if the thread is accessing a node for which there is an obstacle. If there exists an obstacle the densities are saved in the \texttt{BOUNCEBACK} variable.

The \texttt{DENSITY} is calculated by summing up the nine matrices for the current node. The same happens for the \texttt{UX} and \texttt{UY} variables. The inlet pressure is increased in the same way as in the Matlab and NumPy version. Here the same test case can be set up as in the NumPy version, called the lid driven cavity. The parameter to change scenario is supplied to the kernel via string replacement. In the kernel \%s denotes the place to insert and on line 202 of \hpref{listing}{lbmcuda} the replacement is done. Lastly the density for the obstacle nodes is set to zero in \texttt{D}, \texttt{UX} and \texttt{UY}. This is done in the end as to avoid divide-by-zero errors when calculating the \texttt{UX} and \texttt{UY} variables.


\paragraph{Equilibrium kernel}
The equilibrium kernel is implemented on lines 204--260 in \hpref{listing}{lbmcuda}. It is not different from the Matlab and PyCUDA version, except from that it is now working on each element individually. It is merely implemented in CUDA to save memory traffic to and from the GPU. In this way all the variables and calculations can reside on the GPU, saving a large overhead there is in copying variables back and forth.

As with the density kernel, constants are supplied to the kernel by string replacement.


\paragraph{Bounceback kernel}
The bounce back kernel works in the same way as the density kernel. The thread uses \texttt{BOUND} to detect if it is an obstacle node. The shifted densities are then reassigned back to \texttt{F}.




\subsection{Correctness}
To test for the correctness of the two implementations in NumPy and PyCUDA, a test was devised involving the original Matlab script. The aim of the test was of course to generate the same output in each of the different versions. A 10 by 10 field with a boundary consisting of one line along the x-axis was used to create the same conditions in each implementation. The implementation of propagation wraps around so it is as if the flow is surrounded vertically in a tube. The Matlab script would generate a unique pattern after 150 iterations, so the test was if the two other implementations would do the same under equal conditions. Here are the results

\begin{figure}[H]
\centering
\subfigure[Matlab]{
\includegraphics[width=0.30\textwidth]{./images/matlabControl.png}
\label{correctness:a}
}
\hspace{1pt}
\subfigure[NumPy]{
\includegraphics[width=0.27\textwidth]{./images/numpyControl.png}
\label{correctness:b}
}
\subfigure[PyCUDA]{
\includegraphics[width=0.27\textwidth]{./images/pycudaControl.png}
\label{correctness:c}
}
\caption{Testing that the three implementations give the same output under equal conditions}
\label{correctness}
\end{figure}

A visual inspection of \hpref{figure}{correctness} reveals that the implementations are correct and produce the expected results according to the original Matlab implementation. There is however a small inaccuracy in the PyCUDA version on \hpref{figure}{correctness:c}, some of the velocities don't have exactly same length and direction as the NumPy version in \hpref{figure}{correctness:b}. This can be attributed to floating point inaccuracy (TODO: cite cuda best practices and maybe others).



\subsection{Optimization}
To further optimize the PyCUDA implementation one would look into different memory structures. The limiting factor in most parallel algorithms is the memory access pattern. Global memory is a relatively slow memory compared with the shared memory or the texture memory. Each of these have their specific purpose, and thought needs to be given on when to use them.

Certain parts could be loaded into memory more suited for the usage of the data. The \texttt{BOUND} variable is for example only read, and would benefit from a memory type which was spatial in layout. The texture memory would be a good candidate for this, and would increase the performance gain over a normal CPU-implementation.

Utilizing shared memory in each block would provide a performance gain also, but this gain would come with the cost of having to synchronize each block with adjacent blocks. Many strategies could be used to resolve this, one very common one is to calculate all changes on the border of the block first so that other blocks would not have to wait for a block to finish. (TODO: find a good quote for this in parallelism book)



\subsection{Issues/bugs}
An issue exists in the kernel launch. The problem arises when trying to launch a kernel larger than the maximum block size (currently set to $16\times16$, and is not a multiple of $16$. If the simulated field is chosen to say for example $25\times25$, the program will throw an error.

The reason why this happens is that the kernel is launched with a fixed block size, and it is the same along the whole grid. Therefore when the kernel is launched, and the allocated memory does not fit with the block size, e.g. the block size is larger than the memory, the program will try to access memory which is not allocated. The real reason for this is that the threads are designed to run on a full block size regardless on the underlying data. The situation is illustrated in figure \hpref{figure}{blockbug}

\insfig{./images/blockbug.pdf}{0.4\textwidth}{The allocated memory is smaller than the designated block size}{blockbug}

The way to remedy this would be to implement a check if the threads in a block are exceeding the data range, and thus have to do a no operation instead of trying to access data which is not there.